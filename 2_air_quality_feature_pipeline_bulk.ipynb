{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4447764c-218b-441a-ab97-4df4062960d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment\n",
      "Added the following directory to the PYTHONPATH: /Users/kcah/Documents/code-repo/air-quality-prediction\n",
      "HopsworksSettings initialized!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def is_google_colab() -> bool:\n",
    "    if \"google.colab\" in str(get_ipython()):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clone_repository() -> None:\n",
    "    !git clone https://github.com/featurestorebook/mlfs-book.git\n",
    "    %cd mlfs-book\n",
    "\n",
    "def install_dependencies() -> None:\n",
    "    !pip install --upgrade uv\n",
    "    !uv pip install --all-extras --system --requirement pyproject.toml\n",
    "\n",
    "if is_google_colab():\n",
    "    clone_repository()\n",
    "    install_dependencies()\n",
    "    root_dir = str(Path().absolute())\n",
    "    print(\"Google Colab environment\")\n",
    "else:\n",
    "    root_dir = Path().absolute()\n",
    "    # Strip ~/notebooks/ccfraud from PYTHON_PATH if notebook started in one of these subdirectories\n",
    "    if root_dir.parts[-1:] == ('airquality',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    if root_dir.parts[-1:] == ('notebooks',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    root_dir = str(root_dir) \n",
    "    print(\"Local environment\")\n",
    "\n",
    "# Add the root directory to the `PYTHONPATH` to use the `recsys` Python module from the notebook.\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "print(f\"Added the following directory to the PYTHONPATH: {root_dir}\")\n",
    "    \n",
    "# Set the environment variables from the file <root_dir>/.env\n",
    "from mlfs import config\n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e46aad",
   "metadata": {},
   "source": [
    "<span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 02: Daily Feature Pipeline for Air Quality (aqicn.org) and weather (openmeteo)</span>\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into the following sections:\n",
    "1. Download and Parse Data\n",
    "2. Feature Group Insertion\n",
    "\n",
    "\n",
    "__This notebook should be scheduled to run daily__\n",
    "\n",
    "In the book, we use a GitHub Action stored here:\n",
    "[.github/workflows/air-quality-daily.yml](https://github.com/featurestorebook/mlfs-book/blob/main/.github/workflows/air-quality-daily.yml)\n",
    "\n",
    "However, you are free to use any Python Orchestration tool to schedule this program to run daily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe638c6",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de2e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import hopsworks\n",
    "from mlfs.airquality import util\n",
    "from mlfs import config\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6081d1",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üåç Get the Sensor URL, Country, City, Street names from Hopsworks </span>\n",
    "\n",
    "__Update the values in the cell below.__\n",
    "\n",
    "__These should be the same values as in notebook 1 - the feature backfill notebook__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70cd57d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-18 11:51:14,751 INFO: Initializing external client\n",
      "2025-11-18 11:51:14,751 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-18 11:51:16,275 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279136\n",
      "Found 4 location(s) to process:\n",
      "  1. kluuvi - helsinki\n",
      "  2. kallio-2 - helsinki\n",
      "  3. mannerheimintie - helsinki\n",
      "  4. vartiokyla-huivipolku - helsinki\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store() \n",
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "# This line will fail if you have not registered the AQICN_API_KEY as a secret in Hopsworks\n",
    "AQICN_API_KEY = secrets.get_secret(\"AQICN_API_KEY\").value\n",
    "location_str = secrets.get_secret(\"SENSOR_LOCATION_HELSINKI_JSON\").value\n",
    "locations = json.loads(location_str)\n",
    "\n",
    "# Handle both single location (dict) and multiple locations (list)\n",
    "if isinstance(locations, dict):\n",
    "    # Single location - convert to list for consistent processing\n",
    "    locations = [locations]\n",
    "elif isinstance(locations, list):\n",
    "    # Already a list\n",
    "    pass\n",
    "else:\n",
    "    raise ValueError(\"SENSOR_LOCATION_HELSINKI_JSON must be either a JSON object or a JSON array\")\n",
    "\n",
    "print(f\"Found {len(locations)} location(s) to process:\")\n",
    "for i, loc in enumerate(locations, 1):\n",
    "    print(f\"  {i}. {loc.get('street', 'unknown')} - {loc.get('city', 'unknown')}\")\n",
    "\n",
    "today = datetime.datetime.today().replace(hour=0, minute=0, second=0, microsecond=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf9289",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> üîÆ Get references to the Feature Groups </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66f5d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature groups\n",
    "air_quality_fg = fs.get_feature_group(\n",
    "    name='air_quality_helsinki',\n",
    "    version=1,\n",
    ")\n",
    "weather_fg = fs.get_feature_group(\n",
    "    name='weather_helsinki',\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b6ce8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7ffa41",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üå´ Retrieve Today's Air Quality data (PM2.5) from the AQI API</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f681af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing location 1/4: kluuvi\n",
      "============================================================\n",
      "Retrieved air quality data for kluuvi\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.73s) \n",
      "Inserting air quality data for kluuvi...\n",
      "2025-11-18 11:51:24,331 INFO: \t1 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279136/fs/1265746/fg/1718962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1/1 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: air_quality_helsinki_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1279136/jobs/named/air_quality_helsinki_1_offline_fg_materialization/executions\n",
      "‚úì Successfully inserted air quality data for kluuvi\n",
      "\n",
      "============================================================\n",
      "Processing location 2/4: kallio-2\n",
      "============================================================\n",
      "Retrieved air quality data for kallio-2\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.65s) \n",
      "Inserting air quality data for kallio-2...\n",
      "2025-11-18 11:51:41,466 INFO: \t1 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279136/fs/1265746/fg/1718962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1/1 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Successfully inserted air quality data for kallio-2\n",
      "\n",
      "============================================================\n",
      "Processing location 3/4: mannerheimintie\n",
      "============================================================\n",
      "Retrieved air quality data for mannerheimintie\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.69s) \n",
      "Inserting air quality data for mannerheimintie...\n",
      "2025-11-18 11:51:52,065 INFO: \t1 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279136/fs/1265746/fg/1718962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1/1 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Successfully inserted air quality data for mannerheimintie\n",
      "\n",
      "============================================================\n",
      "Processing location 4/4: vartiokyla-huivipolku\n",
      "============================================================\n",
      "Retrieved air quality data for vartiokyla-huivipolku\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.75s) \n",
      "Inserting air quality data for vartiokyla-huivipolku...\n",
      "2025-11-18 11:52:03,175 INFO: \t1 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279136/fs/1265746/fg/1718962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1/1 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Successfully inserted air quality data for vartiokyla-huivipolku\n",
      "\n",
      "============================================================\n",
      "Summary: Processed 4 location(s)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from datetime import timezone\n",
    "\n",
    "# Process each location\n",
    "all_aq_data = []  # Store all air quality dataframes\n",
    "weather_processed = False  # Weather data only needs to be processed once (same for all locations)\n",
    "\n",
    "for location_idx, location in enumerate(locations, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing location {location_idx}/{len(locations)}: {location['street']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Extract location variables\n",
    "    country = location['country']\n",
    "    city = location['city']\n",
    "    street = location['street']\n",
    "    aqicn_url = location['aqicn_url']\n",
    "    latitude = location['latitude']\n",
    "    longitude = location['longitude']\n",
    "    \n",
    "    # Retrieve today's air quality data\n",
    "    aq_today_df = util.get_pm25(aqicn_url, country, city, street, today, AQICN_API_KEY)\n",
    "    print(f\"Retrieved air quality data for {street}\")\n",
    "    \n",
    "    # Add lagged features\n",
    "    # Get dates for 1, 2, and 3 days ago, set to midnight and normalize to UTC\n",
    "    today_ts = pd.Timestamp.now(tz='UTC')\n",
    "    date_1d_ago = (today_ts - timedelta(days=1)).astimezone(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    date_2d_ago = (today_ts - timedelta(days=2)).astimezone(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    date_3d_ago = (today_ts - timedelta(days=3)).astimezone(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    date_4d_ago = (today_ts - timedelta(days=4)).astimezone(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    \n",
    "    # Initialize lagged features as None\n",
    "    aq_today_df['pm25_lag_1d'] = None\n",
    "    aq_today_df['pm25_lag_2d'] = None\n",
    "    aq_today_df['pm25_lag_3d'] = None\n",
    "    \n",
    "    # Try to retrieve previous days' data from the feature store\n",
    "    try:\n",
    "        # Query for previous days' pm25 values\n",
    "        historical_data = air_quality_fg.filter(\n",
    "            (air_quality_fg.country == country) &\n",
    "            (air_quality_fg.city == city) &\n",
    "            (air_quality_fg.street == street) &\n",
    "            (air_quality_fg.date >= date_4d_ago) &\n",
    "            (air_quality_fg.date < today_ts)\n",
    "        ).read()\n",
    "        \n",
    "        # Select only the columns we need\n",
    "        historical_data = historical_data[['date', 'pm25']]\n",
    "        \n",
    "        # Sort by date\n",
    "        historical_data = historical_data.sort_values('date')\n",
    "        \n",
    "        # Normalize historical_data dates to UTC to match comparison timestamps\n",
    "        if historical_data['date'].dt.tz is not None:\n",
    "            historical_data['date'] = historical_data['date'].dt.tz_convert('UTC')\n",
    "        \n",
    "        # Get the pm25 values for 1, 2, and 3 days ago\n",
    "        for idx, row in aq_today_df.iterrows():\n",
    "            # Convert dates to pandas Timestamps and normalize to UTC for comparison\n",
    "            ts_1d_ago = pd.Timestamp(date_1d_ago).tz_convert('UTC')\n",
    "            ts_2d_ago = pd.Timestamp(date_2d_ago).tz_convert('UTC')\n",
    "            ts_3d_ago = pd.Timestamp(date_3d_ago).tz_convert('UTC')\n",
    "            \n",
    "            # Get pm25 for 1 day ago\n",
    "            lag_1d = historical_data[historical_data['date'] == ts_1d_ago]\n",
    "            if not lag_1d.empty:\n",
    "                aq_today_df.at[idx, 'pm25_lag_1d'] = lag_1d['pm25'].iloc[0]\n",
    "            \n",
    "            # Get pm25 for 2 days ago\n",
    "            lag_2d = historical_data[historical_data['date'] == ts_2d_ago]\n",
    "            if not lag_2d.empty:\n",
    "                aq_today_df.at[idx, 'pm25_lag_2d'] = lag_2d['pm25'].iloc[0]\n",
    "            \n",
    "            lag_3d = historical_data[historical_data['date'] == date_3d_ago]\n",
    "            if not lag_3d.empty:\n",
    "                aq_today_df.at[idx, 'pm25_lag_3d'] = lag_3d['pm25'].iloc[0]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not retrieve lagged features for {street}: {e}\")\n",
    "        print(\"Lagged features will be set to None for this day\")\n",
    "    \n",
    "    # Convert lagged features to float32, handling None values\n",
    "    aq_today_df['pm25_lag_1d'] = pd.to_numeric(aq_today_df['pm25_lag_1d'], errors='coerce').astype('float32')\n",
    "    aq_today_df['pm25_lag_2d'] = pd.to_numeric(aq_today_df['pm25_lag_2d'], errors='coerce').astype('float32')\n",
    "    aq_today_df['pm25_lag_3d'] = pd.to_numeric(aq_today_df['pm25_lag_3d'], errors='coerce').astype('float32')\n",
    "    \n",
    "    # Store for later insertion\n",
    "    all_aq_data.append(aq_today_df)\n",
    "    \n",
    "    # Insert air quality data for this location\n",
    "    print(f\"Inserting air quality data for {street}...\")\n",
    "    air_quality_fg.insert(aq_today_df)\n",
    "    print(f\"‚úì Successfully inserted air quality data for {street}\")\n",
    "\n",
    "# Combine all air quality dataframes for display\n",
    "if all_aq_data:\n",
    "    combined_aq_df = pd.concat(all_aq_data, ignore_index=True)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Summary: Processed {len(all_aq_data)} location(s)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    combined_aq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9e24eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   pm25         4 non-null      float32       \n",
      " 1   country      4 non-null      object        \n",
      " 2   city         4 non-null      object        \n",
      " 3   street       4 non-null      object        \n",
      " 4   date         4 non-null      datetime64[us]\n",
      " 5   url          4 non-null      object        \n",
      " 6   pm25_lag_1d  4 non-null      float32       \n",
      " 7   pm25_lag_2d  4 non-null      float32       \n",
      " 8   pm25_lag_3d  4 non-null      float32       \n",
      "dtypes: datetime64[us](1), float32(4), object(4)\n",
      "memory usage: 356.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# Display info for the combined air quality dataframe\n",
    "if 'combined_aq_df' in locals():\n",
    "    combined_aq_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49023549",
   "metadata": {},
   "source": [
    "## Add Lagged Air Quality Features\n",
    "\n",
    "We will retrieve the previous 1, 2, and 3 days of air quality data from the feature store and add them as lagged features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ce8f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pm25  country      city                 street       date  \\\n",
      "0  21.0  finland  helsinki                 kluuvi 2025-11-18   \n",
      "1   5.0  finland  helsinki               kallio-2 2025-11-18   \n",
      "2  21.0  finland  helsinki        mannerheimintie 2025-11-18   \n",
      "3   5.0  finland  helsinki  vartiokyla-huivipolku 2025-11-18   \n",
      "\n",
      "                                url  pm25_lag_1d  pm25_lag_2d  pm25_lag_3d  \n",
      "0  https://api.waqi.info/feed/@5717          5.0          9.0         13.0  \n",
      "1  https://api.waqi.info/feed/@4908         13.0          8.0          7.0  \n",
      "2  https://api.waqi.info/feed/@4909         16.0          9.0         13.0  \n",
      "3  https://api.waqi.info/feed/@4910         10.0          8.0          7.0  \n"
     ]
    }
   ],
   "source": [
    "# Select today's date for each street with all columns in combined_aq_df\n",
    "if 'combined_aq_df' in locals():\n",
    "    today = pd.Timestamp.today().normalize()\n",
    "    # Assume 'date' is either datetime64[ns] or string in YYYY-MM-DD format, coerce to date for comparison\n",
    "    filtered_today_df = combined_aq_df[\n",
    "        pd.to_datetime(combined_aq_df['date']).dt.normalize() == today\n",
    "    ]\n",
    "    print(filtered_today_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b75d5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.10s) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>pm25</th>\n",
       "      <th>street</th>\n",
       "      <th>url</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>pm25_lag_1d</th>\n",
       "      <th>pm25_lag_2d</th>\n",
       "      <th>pm25_lag_3d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>2025-11-17 00:00:00+00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>kluuvi</td>\n",
       "      <td>https://api.waqi.info/feed/@5717</td>\n",
       "      <td>finland</td>\n",
       "      <td>helsinki</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3087</th>\n",
       "      <td>2025-11-16 00:00:00+00:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>kluuvi</td>\n",
       "      <td>https://api.waqi.info/feed/@5717</td>\n",
       "      <td>finland</td>\n",
       "      <td>helsinki</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2724</th>\n",
       "      <td>2025-11-15 00:00:00+00:00</td>\n",
       "      <td>13.0</td>\n",
       "      <td>kluuvi</td>\n",
       "      <td>https://api.waqi.info/feed/@5717</td>\n",
       "      <td>finland</td>\n",
       "      <td>helsinki</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>2025-11-14 00:00:00+00:00</td>\n",
       "      <td>13.0</td>\n",
       "      <td>kluuvi</td>\n",
       "      <td>https://api.waqi.info/feed/@5717</td>\n",
       "      <td>finland</td>\n",
       "      <td>helsinki</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>2025-11-13 00:00:00+00:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>kluuvi</td>\n",
       "      <td>https://api.waqi.info/feed/@5717</td>\n",
       "      <td>finland</td>\n",
       "      <td>helsinki</td>\n",
       "      <td>28.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>2025-11-12 00:00:00+00:00</td>\n",
       "      <td>28.0</td>\n",
       "      <td>kluuvi</td>\n",
       "      <td>https://api.waqi.info/feed/@5717</td>\n",
       "      <td>finland</td>\n",
       "      <td>helsinki</td>\n",
       "      <td>30.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520</th>\n",
       "      <td>2025-11-11 00:00:00+00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>kluuvi</td>\n",
       "      <td>https://api.waqi.info/feed/@5717</td>\n",
       "      <td>finland</td>\n",
       "      <td>helsinki</td>\n",
       "      <td>17.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>2025-11-10 00:00:00+00:00</td>\n",
       "      <td>17.0</td>\n",
       "      <td>kluuvi</td>\n",
       "      <td>https://api.waqi.info/feed/@5717</td>\n",
       "      <td>finland</td>\n",
       "      <td>helsinki</td>\n",
       "      <td>38.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112</th>\n",
       "      <td>2025-11-09 00:00:00+00:00</td>\n",
       "      <td>38.0</td>\n",
       "      <td>kluuvi</td>\n",
       "      <td>https://api.waqi.info/feed/@5717</td>\n",
       "      <td>finland</td>\n",
       "      <td>helsinki</td>\n",
       "      <td>45.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4152</th>\n",
       "      <td>2025-11-08 00:00:00+00:00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>kluuvi</td>\n",
       "      <td>https://api.waqi.info/feed/@5717</td>\n",
       "      <td>finland</td>\n",
       "      <td>helsinki</td>\n",
       "      <td>28.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          date  pm25  street  \\\n",
       "1881 2025-11-17 00:00:00+00:00   5.0  kluuvi   \n",
       "3087 2025-11-16 00:00:00+00:00   9.0  kluuvi   \n",
       "2724 2025-11-15 00:00:00+00:00  13.0  kluuvi   \n",
       "1609 2025-11-14 00:00:00+00:00  13.0  kluuvi   \n",
       "387  2025-11-13 00:00:00+00:00  18.0  kluuvi   \n",
       "1031 2025-11-12 00:00:00+00:00  28.0  kluuvi   \n",
       "2520 2025-11-11 00:00:00+00:00  30.0  kluuvi   \n",
       "2460 2025-11-10 00:00:00+00:00  17.0  kluuvi   \n",
       "3112 2025-11-09 00:00:00+00:00  38.0  kluuvi   \n",
       "4152 2025-11-08 00:00:00+00:00  45.0  kluuvi   \n",
       "\n",
       "                                   url  country      city  pm25_lag_1d  \\\n",
       "1881  https://api.waqi.info/feed/@5717  finland  helsinki          9.0   \n",
       "3087  https://api.waqi.info/feed/@5717  finland  helsinki         13.0   \n",
       "2724  https://api.waqi.info/feed/@5717  finland  helsinki         13.0   \n",
       "1609  https://api.waqi.info/feed/@5717  finland  helsinki         18.0   \n",
       "387   https://api.waqi.info/feed/@5717  finland  helsinki         28.0   \n",
       "1031  https://api.waqi.info/feed/@5717  finland  helsinki         30.0   \n",
       "2520  https://api.waqi.info/feed/@5717  finland  helsinki         17.0   \n",
       "2460  https://api.waqi.info/feed/@5717  finland  helsinki         38.0   \n",
       "3112  https://api.waqi.info/feed/@5717  finland  helsinki         45.0   \n",
       "4152  https://api.waqi.info/feed/@5717  finland  helsinki         28.0   \n",
       "\n",
       "      pm25_lag_2d  pm25_lag_3d  \n",
       "1881         13.0         13.0  \n",
       "3087         13.0         18.0  \n",
       "2724         18.0         28.0  \n",
       "1609         28.0         30.0  \n",
       "387          30.0         17.0  \n",
       "1031         17.0         38.0  \n",
       "2520         38.0         45.0  \n",
       "2460         45.0         28.0  \n",
       "3112         28.0         32.0  \n",
       "4152         32.0         21.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "makelankatu_aq_df = air_quality_fg.filter(\n",
    "    air_quality_fg.street == \"kluuvi\"\n",
    ").read().sort_values('date', ascending=False)\n",
    "makelankatu_aq_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af845ab6",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üå¶ Get Weather Forecast data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2ecb3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing weather data for helsinki (latitude: 60.1733244, longitude: 24.9410248)\n",
      "Coordinates 60.25¬∞N 25.0¬∞E\n",
      "Elevation 2.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "‚úì Weather data processed for helsinki\n"
     ]
    }
   ],
   "source": [
    "# Weather data is the same for all locations in the same city, so process it only once\n",
    "# Use the first location's city and coordinates (all locations should have the same city)\n",
    "if locations:\n",
    "    first_location = locations[0]\n",
    "    city = first_location['city']\n",
    "    latitude = first_location['latitude']\n",
    "    longitude = first_location['longitude']\n",
    "    \n",
    "    print(f\"Processing weather data for {city} (latitude: {latitude}, longitude: {longitude})\")\n",
    "    \n",
    "    start_date = pd.Timestamp.today().normalize()\n",
    "    end_date = start_date + pd.Timedelta(days=10)\n",
    "    # Convert dates to 'YYYY-MM-DD' format strings\n",
    "    start_date = start_date.strftime('%Y-%m-%d')\n",
    "    end_date = end_date.strftime('%Y-%m-%d')\n",
    "    hourly_df = util.get_hourly_weather_forecast(city, latitude, longitude, start_date, end_date)\n",
    "    hourly_df = hourly_df.set_index('date')\n",
    "    \n",
    "    # We will only make 1 daily prediction, so we will replace the hourly forecasts with a single daily forecast\n",
    "    # We only want the daily weather data, so only get weather at 12:00\n",
    "    daily_df = hourly_df.between_time('11:59', '12:01')\n",
    "    daily_df = daily_df.reset_index()\n",
    "    daily_df['date'] = pd.to_datetime(daily_df['date']).dt.date\n",
    "    daily_df['date'] = pd.to_datetime(daily_df['date'])\n",
    "    daily_df['city'] = city\n",
    "    \n",
    "    print(f\"‚úì Weather data processed for {city}\")\n",
    "    daily_df\n",
    "else:\n",
    "    print(\"No locations found. Cannot process weather data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3793778c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature_2m_mean</th>\n",
       "      <th>precipitation_sum</th>\n",
       "      <th>wind_speed_10m_max</th>\n",
       "      <th>wind_direction_10m_dominant</th>\n",
       "      <th>rain_sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-11-18 00:00:00</th>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.565437</td>\n",
       "      <td>340.201019</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-18 01:00:00</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.832680</td>\n",
       "      <td>336.250488</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-18 02:00:00</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.315115</td>\n",
       "      <td>330.751282</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-18 03:00:00</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.630672</td>\n",
       "      <td>331.699341</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-18 04:00:00</th>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.931203</td>\n",
       "      <td>342.758453</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-28 19:00:00</th>\n",
       "      <td>-5.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.268990</td>\n",
       "      <td>56.592163</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-28 20:00:00</th>\n",
       "      <td>-5.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.267441</td>\n",
       "      <td>56.040897</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-28 21:00:00</th>\n",
       "      <td>-5.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.971800</td>\n",
       "      <td>55.007900</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-28 22:00:00</th>\n",
       "      <td>-5.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.682856</td>\n",
       "      <td>54.039394</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-28 23:00:00</th>\n",
       "      <td>-4.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.400000</td>\n",
       "      <td>53.130020</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     temperature_2m_mean  precipitation_sum  \\\n",
       "date                                                          \n",
       "2025-11-18 00:00:00                -0.20                0.0   \n",
       "2025-11-18 01:00:00                -0.00                0.0   \n",
       "2025-11-18 02:00:00                 0.15                0.0   \n",
       "2025-11-18 03:00:00                 0.10                0.0   \n",
       "2025-11-18 04:00:00                -0.25                0.0   \n",
       "...                                  ...                ...   \n",
       "2025-11-28 19:00:00                -5.10                0.0   \n",
       "2025-11-28 20:00:00                -5.15                0.0   \n",
       "2025-11-28 21:00:00                -5.10                0.0   \n",
       "2025-11-28 22:00:00                -5.05                0.0   \n",
       "2025-11-28 23:00:00                -4.95                0.0   \n",
       "\n",
       "                     wind_speed_10m_max  wind_direction_10m_dominant  rain_sum  \n",
       "date                                                                            \n",
       "2025-11-18 00:00:00            9.565437                   340.201019       0.0  \n",
       "2025-11-18 01:00:00            9.832680                   336.250488       0.0  \n",
       "2025-11-18 02:00:00           10.315115                   330.751282       0.0  \n",
       "2025-11-18 03:00:00           10.630672                   331.699341       0.0  \n",
       "2025-11-18 04:00:00           10.931203                   342.758453       0.0  \n",
       "...                                 ...                          ...       ...  \n",
       "2025-11-28 19:00:00           20.268990                    56.592163       0.0  \n",
       "2025-11-28 20:00:00           21.267441                    56.040897       0.0  \n",
       "2025-11-28 21:00:00           21.971800                    55.007900       0.0  \n",
       "2025-11-28 22:00:00           22.682856                    54.039394       0.0  \n",
       "2025-11-28 23:00:00           23.400000                    53.130020       0.0  \n",
       "\n",
       "[264 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c563109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11 entries, 0 to 10\n",
      "Data columns (total 7 columns):\n",
      " #   Column                       Non-Null Count  Dtype         \n",
      "---  ------                       --------------  -----         \n",
      " 0   date                         11 non-null     datetime64[ns]\n",
      " 1   temperature_2m_mean          11 non-null     float32       \n",
      " 2   precipitation_sum            11 non-null     float32       \n",
      " 3   wind_speed_10m_max           11 non-null     float32       \n",
      " 4   wind_direction_10m_dominant  11 non-null     float32       \n",
      " 5   rain_sum                     11 non-null     float32       \n",
      " 6   city                         11 non-null     object        \n",
      "dtypes: datetime64[ns](1), float32(5), object(1)\n",
      "memory usage: 528.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "daily_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f5008",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:#ff5f27;\">‚¨ÜÔ∏è Uploading new data to the Feature Store</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a9de5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Air quality data insertion is now handled within the main loop in cell 11 above\n",
    "# This cell is kept for reference but is no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d491b0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting weather data...\n",
      "2025-11-18 11:57:03,073 INFO: \t2 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279136/fs/1265746/fg/1721946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 11/11 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: weather_helsinki_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1279136/jobs/named/weather_helsinki_1_offline_fg_materialization/executions\n",
      "2025-11-18 11:57:20,180 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2025-11-18 11:57:26,503 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2025-11-18 11:58:42,828 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2025-11-18 11:58:42,978 INFO: Waiting for log aggregation to finish.\n",
      "2025-11-18 11:58:51,547 INFO: Execution finished successfully.\n",
      "‚úì Successfully inserted weather data\n"
     ]
    }
   ],
   "source": [
    "# Insert weather data (only needs to be done once since all locations share the same city)\n",
    "if 'daily_df' in locals() and daily_df is not None:\n",
    "    print(\"Inserting weather data...\")\n",
    "    weather_fg.insert(daily_df, wait=True)\n",
    "    print(\"‚úì Successfully inserted weather data\")\n",
    "else:\n",
    "    print(\"Warning: daily_df not found. Weather data was not inserted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e9e2d",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">‚è≠Ô∏è **Next:** Part 03: Training Pipeline\n",
    " </span> \n",
    "\n",
    "In the following notebook you will read from a feature group and create training dataset within the feature store\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
